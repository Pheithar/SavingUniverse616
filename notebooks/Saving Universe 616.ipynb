{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc5f0d18",
   "metadata": {},
   "source": [
    "# Saving Universe 616\n",
    "\n",
    "This project aims to analyze a graph made by characters and teams from the [Marvel Earth-616](https://marvel.fandom.com/wiki/Earth-616). Being the central Marvel universe, it has a lot of different characters and teams (30021 other characters).\n",
    "\n",
    "![header](../images/marvel616.webp)\n",
    "\n",
    "The aim is to offer some understanding of the relationships inside this vast universe and try to get meaningful information from it.\n",
    "\n",
    "The data is obtained from the [official marver wiki](https://marvel.fandom.com/wiki/Marvel_Database).\n",
    "\n",
    "\n",
    "## Motivation\n",
    "\n",
    "The Marvel multiverse is formed by all the marvel comics, animated series and films that uses the Marvel characters. As you can imagine, it is pretty huge multiverse. Thuis, is divided in universes, defined by the word `Earth` followed by a number, that works as a universe ID, for example `Earth-616` is the main marvel universe, where the most famous Marvel events happened, such as Civil War, Infinity Saga or the House of M.\n",
    "\n",
    "We are focusing in this universe, as is the one with the most charcaters and most interesting stories.\n",
    "\n",
    "The motivation of the work is to try to analyze and get information from the characters in the Earth-616 that belong to, at least, one team.\n",
    "\n",
    "This limitation is due to how many characters there are, in an attempt to reduce the size of our netwrok. \n",
    "\n",
    "\n",
    "## Basic stats\n",
    "\n",
    "At first, we worked with just a few characters, because we had to manually select a couple teams, and we only got the characters that belonged to that team. After some more research, we were able to get every single team and orgnaization, but we had to do some more complex queries. \n",
    "\n",
    "We had to specify that the `list` is `categorymember` What this does is it gets all the elements that belong to a category. In this case the category is `Category:Earth-616/Teams` or `Category:Earth-616/Organizations`. This is because we only want the teams that belong to the universe 616, qhich is the main marvel universe. That way we can get all the teams. And luckily, there is another category that is very useful to us, which is `Category:TEAM/Members`, which returns as a list all the members of a single team. That way we can get all teams and all characters that belong to at least a team from the marvel 616 universe.\n",
    "\n",
    "There is just one small problem. The maximun elements that `categorymember` return is 500, and there are 3054 teams and 19148 elements. To solve this there is the `cmcontinue` parameter, which allows to get the next 500 elements. This eleme nt is returned by the query, so to get every element from the list, we need to do a while loop until the `cmcontinue` value is empty, because it means there are no more values to retrieve with the query.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c9bd1f",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "What would be of a project without some imports ;)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725af387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For API queries\n",
    "import urllib\n",
    "import json\n",
    "\n",
    "# For fancy looking loops\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# For transforming to literal value instead of string (\"[a, b, c]\" => [\"a\", \"b\", \"c\"])\n",
    "import ast\n",
    "\n",
    "# For os operations (duh)\n",
    "import os\n",
    "\n",
    "# Small file that have useful functions\n",
    "import utils\n",
    "\n",
    "# For multithreading\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# For regex operations\n",
    "import re\n",
    "\n",
    "# For sentiment analysis\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# For plot visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For wordclouds\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# For natural language processing\n",
    "import nltk\n",
    "\n",
    "# For graph building\n",
    "import networkx as nx\n",
    "\n",
    "# For numeric transformations and operations\n",
    "import numpy as np\n",
    "\n",
    "# For calculating the powerlaw\n",
    "import powerlaw\n",
    "\n",
    "# For more iterating tools\n",
    "import itertools\n",
    "\n",
    "# For the fa2 algorithm\n",
    "from fa2 import ForceAtlas2\n",
    "\n",
    "# For randomness\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddd2af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So tqdm works with pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# Cool default plots\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf70228",
   "metadata": {},
   "source": [
    "## Teams\n",
    "\n",
    "We will start by analyzing and getting information from the teams. We use `Category:Earth-616/Teams` from the wiki as it is where the teams are. Unfortunately, it is not the only place that information is. We realized that superhero teams, such as the **Avengers**, are not actually a team, but an organization, so we also need to use `Category:Earth-616/Organizations`.\n",
    "\n",
    "The function is quite lengthy and maybe a bit convoluted, so we try to explain it as clearly as possible (note that all the steps are repeated for `Teams` and `Organization):\n",
    "\n",
    "The query we do is something like:\n",
    "\n",
    "```\n",
    "https://marvel.fandom.com/api.php?action=query&list=categorymembers&cmtitle=Category:Earth-616/GROUP&prop=revisions&rvprop=content&rvslots=*&format=json&cmlimit=max&cmcontinue=CONTINUE_CODE\"\n",
    "```\n",
    "\n",
    "We know it is very long, but it has some fundamental values: `list=categorymembers`, which returns all elements that belong to a category, in this case, `Team` or `Organization`, and `cmcontinue=CONTINUE_CODE`, which allows us to do a for loop over multiple queries, as all the teams do not fit in the 500 limit the query has. Because we do not know how many there are, a `while` loop is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e57ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_teams():\n",
    "  \n",
    "  cmcontinue_text = \"\"\n",
    "  first_time = True\n",
    "  \n",
    "  team_list = []\n",
    "  \n",
    "  while cmcontinue_text or first_time: \n",
    "  \n",
    "    first_time = False\n",
    "  \n",
    "    baseurl = \"https://marvel.fandom.com/api.php?\"\n",
    "    action = \"action=query&list=categorymembers\"\n",
    "    q_title = \"cmtitle=Category:Earth-616/Teams\"\n",
    "\n",
    "    content = \"prop=revisions&rvprop=content&rvslots=*\"\n",
    "    dataformat =\"format=json\"\n",
    "    cmcontinue = \"cmlimit=max&cmcontinue={}\".format(cmcontinue_text)\n",
    "\n",
    "    query = \"{}{}&{}&{}&{}&{}\".format(baseurl, action, q_title, content, dataformat, cmcontinue)\n",
    "    wikiresponse = urllib.request.urlopen(query)\n",
    "    wikidata = wikiresponse.read()\n",
    "    wikitext = wikidata.decode('utf-8')\n",
    "    \n",
    "    wiki_json = json.loads(wikitext)\n",
    "    \n",
    "    team_list += [team[\"title\"] for team in wiki_json[\"query\"][\"categorymembers\"]\n",
    "                 if not team[\"title\"].startswith(\"Category:\")]\n",
    "    \n",
    "    if \"continue\" in list(wiki_json.keys()):\n",
    "      cmcontinue_text = wiki_json[\"continue\"][\"cmcontinue\"]\n",
    "    else:\n",
    "      cmcontinue_text = \"\"\n",
    "      \n",
    "  first_time = True\n",
    "    \n",
    "  while cmcontinue_text or first_time: \n",
    "  \n",
    "    first_time = False\n",
    "  \n",
    "    baseurl = \"https://marvel.fandom.com/api.php?\"\n",
    "    action = \"action=query&list=categorymembers\"\n",
    "    q_title = \"cmtitle=Category:Earth-616/Organizations\"\n",
    "\n",
    "    content = \"prop=revisions&rvprop=content&rvslots=*\"\n",
    "    dataformat =\"format=json\"\n",
    "    cmcontinue = \"cmlimit=max&cmcontinue={}\".format(cmcontinue_text)\n",
    "\n",
    "    query = \"{}{}&{}&{}&{}&{}\".format(baseurl, action, q_title, content, dataformat, cmcontinue)\n",
    "    wikiresponse = urllib.request.urlopen(query)\n",
    "    wikidata = wikiresponse.read()\n",
    "    wikitext = wikidata.decode('utf-8')\n",
    "    \n",
    "    wiki_json = json.loads(wikitext)\n",
    "    \n",
    "    team_list += [team[\"title\"] for team in wiki_json[\"query\"][\"categorymembers\"]\n",
    "                 if not team[\"title\"].startswith(\"Category:\")]\n",
    "    \n",
    "    if \"continue\" in list(wiki_json.keys()):\n",
    "      cmcontinue_text = wiki_json[\"continue\"][\"cmcontinue\"]\n",
    "    else:\n",
    "      cmcontinue_text = \"\"\n",
    "      \n",
    "  return team_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be760c4",
   "metadata": {},
   "source": [
    "Now, with the massive function, we can get all the teams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b28200",
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = list(set(get_teams()))\n",
    "print(f\"There are {len(teams)} teams.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a0587a",
   "metadata": {},
   "source": [
    "### Getting members\n",
    "\n",
    "Now that we have all our teams, we need to get the members of each team.  Luckily, there is a particular page where we can get that: `Category:TEAM_NAME/Members`.\n",
    "\n",
    "The query is very similar to the previous one: \n",
    "\n",
    "```\n",
    "https://marvel.fandom.com/api.php?action=query&list=categorymembers&cmtitle=Category:TEAM_NAME/Members&rop=revisions&rvprop=content&rvslots=*&format=json&cmlimit=max&cmcontinue=CONTINUE_CODE\n",
    "```\n",
    "\n",
    "Which works pretty much the same as the previous one.\n",
    "\n",
    "The only remark is that, for some reason, some teams have 'members' that are actually references to files, so we do not append any character that starts with the keyword `File:`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37624136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMembers(team):  \n",
    "  cmcontinue_text = \"\"\n",
    "  first_time = True\n",
    "  \n",
    "  member_list = []\n",
    "  \n",
    "  while cmcontinue_text or first_time: \n",
    "  \n",
    "    first_time = False\n",
    "  \n",
    "    baseurl = \"https://marvel.fandom.com/api.php?\"\n",
    "    action = \"action=query&list=categorymembers\"\n",
    "    q_title = \"cmtitle=Category:{}/Members\".format(urllib.parse.quote_plus(team.replace(\" \", \"_\")))\n",
    "\n",
    "    content = \"prop=revisions&rvprop=content&rvslots=*\"\n",
    "    dataformat =\"format=json\"\n",
    "    cmcontinue = \"cmlimit=max&cmcontinue={}\".format(cmcontinue_text)\n",
    "\n",
    "    query = \"{}{}&{}&{}&{}&{}\".format(baseurl, action, q_title, content, dataformat, cmcontinue)\n",
    "    wikiresponse = urllib.request.urlopen(query)\n",
    "    wikidata = wikiresponse.read()\n",
    "    wikitext = wikidata.decode('utf-8')\n",
    "    \n",
    "    wiki_json = json.loads(wikitext)\n",
    "    \n",
    "    member_list += [member[\"title\"]\n",
    "                    for member in wiki_json[\"query\"][\"categorymembers\"]\n",
    "                    if not member[\"title\"].startswith(\"File:\")]\n",
    "    \n",
    "    if \"continue\" in list(wiki_json.keys()):\n",
    "      cmcontinue_text = wiki_json[\"continue\"][\"cmcontinue\"]\n",
    "    else:\n",
    "      cmcontinue_text = \"\"\n",
    "      \n",
    "  return member_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b913ca1",
   "metadata": {},
   "source": [
    "The next operation is quite lenghty, so if you have the dataframe already, press enter without input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7a8234",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = input(\"Do you want to get the members of the team? \")\n",
    "\n",
    "if answer:\n",
    "  dataset = []\n",
    "  for team in tqdm(teams):\n",
    "    dataset.append([team, getMembers(team)])\n",
    "\n",
    "  df = pd.DataFrame(dataset, columns=[\"team_name\", \"members\"])\n",
    "  df.to_csv(\"../data/marvel_teams.csv\", index=False)\n",
    "else:\n",
    "  df = pd.read_csv(\"../data/marvel_teams.csv\")\n",
    "  df[\"members\"] = df[\"members\"].apply(ast.literal_eval)\n",
    "  \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271f798d",
   "metadata": {},
   "source": [
    "### Information from members\n",
    "\n",
    "Cool right? Now that we have the members of each team, we can start doing fancy stuff. Let's start by getting how many members each team has, and let's check the top 10 teams with more members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3155c498",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"number_members\"] = df[\"members\"].apply(len)\n",
    "\n",
    "df.sort_values(\"number_members\", ascending=False).reset_index(drop=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a6460d",
   "metadata": {},
   "source": [
    "Ok, this is useful and entirely unexpected. The team with the second most members is not a superhero team, but a police department, and not only that, but the third one is the german nazi party and fourth the US army.\n",
    "\n",
    "It makes sense, as each of those has, or had, a lot of real-life members that probably make appearances in the comics and thus are in the wiki.\n",
    "\n",
    "Let's try to see how many different characters there are in total in our dataset right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e9beb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_diff_char = len(list(set(df['members'].sum())))\n",
    "\n",
    "print(f\"There are {len_diff_char} different characters.\")\n",
    "print(f\"This means there are around {30021 - len_diff_char} characters that do not belong to any team.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06c2f0c",
   "metadata": {},
   "source": [
    "To make the network analysis more manageable, we will only work with those characters that belong to at least one team.\n",
    "\n",
    "Well, let's try to see more data from our teams. We can start with some fancy analysis and expand with some sentiment analysis. For that, we can use the quotes of each of the characters of a team, but that can get messy if we do it multiple times for each character (as one character can belong to various teams), so let's try to create a character data frame, and come back to the sentiment analysis for the teams for later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71db0948",
   "metadata": {},
   "source": [
    "## Characters\n",
    "\n",
    "### Create data frame\n",
    "\n",
    "The first step is to create the data frame with all the characters and which teams they belong to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a577acf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_characters = []\n",
    "for _, row in df.iterrows():\n",
    "  for member in row[\"members\"]:\n",
    "    all_characters += [(member, row[\"team_name\"])]\n",
    "    \n",
    "print(f\"Number of characters: {len(all_characters)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec49103",
   "metadata": {},
   "source": [
    "With each character with the team they belong to, we can do a `groupby` and get all the teams in a list and quickly get how many teams each character belongs to.\n",
    "\n",
    "Let's check the top ten characters that belong to more teams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dda6d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char = pd.DataFrame(all_characters, columns=[\"name\", \"team\"])\n",
    "\n",
    "df_char = df_char.groupby(\"name\")[\"team\"].progress_apply(list).to_frame(\"teams\").reset_index()\n",
    "df_char[\"number_teams\"] = df_char[\"teams\"].progress_apply(len)\n",
    "\n",
    "df_char.sort_values(\"number_teams\", ascending=False).reset_index(drop=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3ad294",
   "metadata": {},
   "source": [
    "As expected, most of those characters are pretty famous (in order: Wolverine, Spider-Man, Iron Man, Storm, Beast, Deadpool, Captain Marvel, Taskmaster, Mystique and Hawkeye)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7450a97",
   "metadata": {},
   "source": [
    "\n",
    "### Quotes \n",
    "\n",
    "We are now going to get the quotes for all the characters.\n",
    "\n",
    "The code is quite messy and difficult to read, but the idea is that we use multiple threads to download the content faster, as we can only query one characters quotes at a time, and we have almost 13.000 characters.\n",
    "\n",
    "The query we do is:\n",
    "\n",
    "```\n",
    "https://marvel.fandom.com/api.php?action=query&list=categorymembers&cmtitle=Category:CHARACTER/Quotes&prop=revisions&rvprop=content&rvslots=*&format=json&cmlimit=max&cmcontinue=CONTINUE_CODE\n",
    "```\n",
    "\n",
    "Which, again, works similar to those already seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea96fa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "quote_path = \"../data/character_quotes/\"\n",
    "\n",
    "def get_character_quotes(name: str):\n",
    "  # This is a little bit hacky, but works\n",
    "  cmcontinue_text = \"\"\n",
    "  quote_titles = []\n",
    "\n",
    "  # Breaks when there are no more cm_continues\n",
    "  while True:\n",
    "    baseurl = \"https://marvel.fandom.com/api.php?\"\n",
    "    args = {\n",
    "      \"action\"      : \"action=query&list=categorymembers\",\n",
    "      \"q_title\"     : \"cmtitle=Category:{}/Quotes\".format(urllib.parse.quote_plus(name.replace(\" \", \"_\"))),\n",
    "      \"content\"     : \"prop=revisions&rvprop=content&rvslots=*\",\n",
    "      \"dataformat\"  : \"format=json\",\n",
    "      \"cmcontinue\"  :  \"cmlimit=max&cmcontinue={}\".format(cmcontinue_text),\n",
    "    }\n",
    "    \n",
    "    query = f\"{baseurl}{'&'.join(args.values())}\"\n",
    "\n",
    "    wikiresponse = urllib.request.urlopen(query)\n",
    "    wikitext = wikiresponse.read().decode('utf-8')\n",
    "    wiki_json = json.loads(wikitext)\n",
    "    \n",
    "    quote_titles += [page[\"title\"] for page in wiki_json[\"query\"][\"categorymembers\"]]\n",
    "\n",
    "    if \"continue\" in list(wiki_json.keys()):\n",
    "      cmcontinue_text = wiki_json[\"continue\"][\"cmcontinue\"]\n",
    "    else: break\n",
    "  \n",
    "  quote_title_chunks = utils.generate_chunks(quote_titles)\n",
    "  quotes = []\n",
    "\n",
    "  for chunk in quote_title_chunks:\n",
    "    quote_data = search_quotes(chunk)\n",
    "    for content in quote_data[\"query\"][\"pages\"].values():\n",
    "      content  = content[\"revisions\"][-1][\"slots\"][\"main\"][\"*\"]\n",
    "      quotes += re.findall(r\"Quotation.*?= (.*?)\\n\", content)\n",
    "  \n",
    "  filename = utils.generate_filename(name)\n",
    "  with open(f\"{quote_path}{filename}.json\", \"w\") as f:\n",
    "    json.dump(quotes, f, indent = 4)\n",
    "\n",
    "def get_chunk_quotes(chunk: list):\n",
    "  for name in chunk:\n",
    "    get_character_quotes(name)\n",
    "  return\n",
    "\n",
    "def get_quotes(names: list, max_workers=16):\n",
    "  files = set(os.listdir(quote_path))\n",
    "  missing_names = list(\n",
    "    filter(lambda x: f\"{utils.generate_filename(x)}.json\" not in files,\n",
    "    names)\n",
    "  )\n",
    "\n",
    "  if len(missing_names) == 0:\n",
    "    print(\"No missign quotes found ðŸ˜Š\")\n",
    "    return\n",
    "  \n",
    "  chunks = utils.generate_chunks(missing_names)\n",
    "  print (f\"Generated {len(chunks)} chunks!\")\n",
    "  with tqdm(total=len(chunks)) as pbar:\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "      futures = [ex.submit(get_chunk_quotes, chunk)\n",
    "                  for chunk in chunks]\n",
    "      for future in as_completed(futures):\n",
    "        pbar.update(1)\n",
    "\n",
    "\n",
    "get_quotes(df_char[\"name\"].values , max_workers=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d066a9c5",
   "metadata": {},
   "source": [
    "Now that we have successfully downloaded all quotes, we will load them in the characters dataset and display the top 10 characters with the most quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdef277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getQuotes(row):\n",
    "  quotes = None\n",
    "  try:\n",
    "    with open(\"../data/character_quotes/\"+utils.generate_filename(row[\"name\"])+\".json\") as f:\n",
    "      quotes = ast.literal_eval(f.read())\n",
    "  except:\n",
    "    quotes = []\n",
    "  \n",
    "  return pd.Series([quotes, len(quotes)])\n",
    "\n",
    "\n",
    "df_char[[\"quotes\", \"number_quotes\"]] = df_char.progress_apply(getQuotes, axis=1)\n",
    "\n",
    "df_char.sort_values(\"number_quotes\", ascending=False).reset_index(drop=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f42c12",
   "metadata": {},
   "source": [
    "Again, most of the most famous characters have the most quotes, being Spider_man the one with the most.\n",
    "\n",
    "## Back with teams\n",
    "\n",
    "### Quotes\n",
    "\n",
    "Now that we have the quotes, we are going to give get the quotes per team, and display the top ten with most quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e30db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getQuotes(row):\n",
    "  quotes = []\n",
    "  for member in row[\"members\"]:\n",
    "    try:\n",
    "      with open(\"../data/character_quotes/\"+member.replace(\" \", \"_\")+\".json\") as f:\n",
    "        quotes += ast.literal_eval(f.read())\n",
    "    except:\n",
    "      quotes += []\n",
    "  \n",
    "  return pd.Series([quotes, len(quotes)])\n",
    "\n",
    "\n",
    "df[[\"quotes\", \"number_quotes\"]] = df.progress_apply(getQuotes, axis=1)\n",
    "\n",
    "df.sort_values(\"number_quotes\", ascending=False).reset_index(drop=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d31e5ad",
   "metadata": {},
   "source": [
    "It is not surprising that the team with the most quotes is the Avengers, as it is one of the biggest and most famous teams.\n",
    "\n",
    "Now that each team has its quotes, we can do some text analysis. For example, we can start with how many words, unique words and lexical richness each team has.\n",
    "\n",
    "For that, first, the links, all `\\n` symbols and any non-alphabetical characters will be removed from the quotes. Lastly, the additional spaces are removed.\n",
    "\n",
    "We will display the top 10 teams with more than 500 unique words with the highest lexical richness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265f52ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLexicalRichness(row):\n",
    "  regex_links = r\"(?:\\{\\{.*?\\}\\}|\\[\\[.*?\\]\\])\"\n",
    "  regex_newline = r\"\\\\n\"\n",
    "  regex_no_alpha = r\"[^a-zA-Z ]\"\n",
    "  regex_aditional_space = r\"\\s\\s+\"\n",
    "  \n",
    "  text = \" \".join(row[\"quotes\"])\n",
    "  text = re.sub(regex_links, \"\", text)\n",
    "  text = re.sub(regex_newline, \" \", text)\n",
    "  text = re.sub(regex_no_alpha, \" \", text)\n",
    "  text = re.sub(regex_aditional_space, \" \", text)\n",
    "  \n",
    "  words = [word for word in text.lower().split(\" \") if len(word) > 1]\n",
    "  \n",
    "  number_words = len(words)\n",
    "  number_unique_words = len(list(set(words)))\n",
    "  lexical_richness = 0\n",
    "  \n",
    "  if number_words > 0:\n",
    "    lexical_richness = number_unique_words/number_words\n",
    "  return pd.Series([number_words, number_unique_words, lexical_richness])\n",
    "\n",
    "df[[\"number_words\",\n",
    "    \"number_unique_words\",\n",
    "    \"lexical_richness\"]] = df.progress_apply(getLexicalRichness, axis=1)\n",
    "\n",
    "df[df[\"number_unique_words\"] > 500].sort_values(\"lexical_richness\", ascending=False)\\\n",
    "                                   .reset_index(drop=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1311895",
   "metadata": {},
   "source": [
    "We can do even more interesting stuff, such as sentiment analysis per team. We are going to use VADER sentiment analysis to get the average feeling of each team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0029e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVaderSentiment(row):\n",
    "  sid_obj = SentimentIntensityAnalyzer()\n",
    "  \n",
    "  happy = []\n",
    "  sad = []\n",
    "  neutral = []\n",
    "  compound = []\n",
    "  category = []\n",
    "  \n",
    "  for quote in row[\"quotes\"]:\n",
    "    sentiment_dict = sid_obj.polarity_scores(quote)\n",
    "    happy.append(sentiment_dict[\"pos\"])\n",
    "    sad.append(sentiment_dict[\"neg\"])\n",
    "    neutral.append(sentiment_dict[\"neu\"])\n",
    "    compound.append(sentiment_dict[\"compound\"])\n",
    "    \n",
    "    if sentiment_dict['compound'] >= 0.05:\n",
    "        category.append(\"Positive\")\n",
    " \n",
    "    elif sentiment_dict['compound'] <= - 0.05:\n",
    "        category.append(\"Negative\")\n",
    " \n",
    "    else:\n",
    "        category.append(\"Neutral\")\n",
    "  if len(row[\"quotes\"]) == 0:\n",
    "    return pd.Series([0, 0, 0, 0, \"Neutral\"])\n",
    "  \n",
    "  happy_val = sum(happy)/len(happy)\n",
    "  sad_val = sum(sad)/len(sad)\n",
    "  neutral_val = sum(neutral)/len(neutral)\n",
    "  compound_val = sum(compound)/len(compound)\n",
    "  category_val = max(category, key=category.count)\n",
    "  return pd.Series([happy_val*100, sad_val*100, neutral_val*100, compound_val, category_val])\n",
    "    \n",
    "\n",
    "df[[\"%happy\",\n",
    "   \"%sad\",\n",
    "   \"%neutral\",\n",
    "   \"compound_sentiment\",\n",
    "   \"overall_category\"]] = df.progress_apply(getVaderSentiment, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfe24d3",
   "metadata": {},
   "source": [
    "Now we can display the top 10 happier teams, and top 10 saddest, among those with at least 500 unqieu words (based on compound value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f80e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"number_unique_words\"] > 500].sort_values(\"compound_sentiment\", ascending=False)\\\n",
    "                                   .reset_index(drop=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adc5ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"number_unique_words\"] > 500].sort_values(\"compound_sentiment\", ascending=True)\\\n",
    "                                   .reset_index(drop=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538df193",
   "metadata": {},
   "source": [
    "### WordClouds\n",
    "\n",
    "After this brief sentiment analysis, we can try to do the cool wordclouds :). We are going to display the wordclouds of the top 20 teams with the most number of unique words.\n",
    "\n",
    "For that first we need to preprocess the quotes. Firts, we are going to remove the links, then the `\\n` and not alphabetical numbers. Then remove additional spaces, and set all the words to lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820c68af",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3db162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "\n",
    "def preprocessQuotes(row):\n",
    "  processed_quotes = []\n",
    "  \n",
    "  regex_links = r\"(?:\\{\\{.*?\\}\\}|\\[\\[.*?\\]\\])\"\n",
    "  regex_newline = r\"\\\\n\"\n",
    "  regex_no_alpha = r\"[^a-zA-Z ]\"\n",
    "  regex_aditional_space = r\"\\s\\s+\"\n",
    "  \n",
    "  for quote in row[\"quotes\"]:\n",
    "    quote = re.sub(regex_links, \" \", quote)\n",
    "    quote = re.sub(regex_newline, \" \", quote)\n",
    "    quote = re.sub(regex_no_alpha, \" \", quote)\n",
    "    quote = re.sub(regex_aditional_space, \" \", quote)\n",
    "    \n",
    "    tokens = tokenizer.tokenize(quote)\n",
    "    \n",
    "    all_words = [x.strip().lower() for x in tokens]\n",
    "    \n",
    "    stop_words = list(nltk.corpus.stopwords.words(\"english\")) # Stopwords\n",
    "    \n",
    "    filtered = [x for x in all_words if x not in stop_words]\n",
    "    \n",
    "    lemmatized = [wnl.lemmatize(w) for w in filtered]\n",
    "    \n",
    "    new_stopwords = [\"im\", \"one\", \"earth\", \"know\", \"im\", \"b\", \"u\"]\n",
    "    \n",
    "    lemmatized = [x for x in lemmatized if x not in new_stopwords]\n",
    "    \n",
    "    processed_quotes.append(lemmatized)\n",
    "  \n",
    "  return processed_quotes\n",
    "\n",
    "df[\"processed_quotes\"] = df.progress_apply(preprocessQuotes, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b169ab",
   "metadata": {},
   "source": [
    "Then with the processed quotes, we can display the wordclouds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3471e9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axarr = plt.subplots(10, 2, figsize=(20, 60))\n",
    "\n",
    "df_quotes = df.sort_values(\"number_unique_words\", ascending=False)\n",
    "\n",
    "word_cloud = WordCloud(max_words=2000,\n",
    "                       background_color=\"white\"\n",
    "                      )\n",
    "\n",
    "for i, ax in enumerate(axarr.flatten()):\n",
    "  content = \" \".join(word\n",
    "                     for quote in df_quotes.iloc[i, 13]\n",
    "                     for word in quote)\n",
    "  ax.imshow(word_cloud.generate(content))\n",
    "  ax.axis(\"off\")\n",
    "  ax.set_title(f\"{df_quotes.iloc[i, 0]}, (Number members: {df_quotes.iloc[i, 2]})\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b745fb",
   "metadata": {},
   "source": [
    "From these wordclouds we can see a clear result. Either all of those teams share one specific theme (`thing`, `time`, `going`...), or all share a couple of characters that make take most of the quotes. Either way, it is not giving too much information.\n",
    "\n",
    "### Graph\n",
    "\n",
    "Now the moment of truth is here. It is time to build the graph. We are going to join two teams if they share at least one member. So the first step is, obviously, get wich teams share a member. For that, we can use the previously build `df_char`, dataframe of characters. We can check which ones are the top ten most connected teams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c1573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getConnections(row):\n",
    "  connections = []\n",
    "  \n",
    "  df_char_row = df_char[df_char[\"name\"].isin(row[\"members\"])]\n",
    "    \n",
    "  for _, row_char in df_char_row.iterrows():\n",
    "    if row[\"team_name\"] in row_char[\"teams\"]:\n",
    "      connections += row_char[\"teams\"]\n",
    "      connections.remove(row[\"team_name\"])\n",
    "      \n",
    "  connections = list(set(connections))\n",
    "  \n",
    "  return pd.Series([connections, len(connections)])\n",
    "\n",
    "df[[\"connections\", \"number_connections\"]] = df.progress_apply(getConnections, axis=1)  \n",
    "\n",
    "df.sort_values(\"number_connections\", ascending=False).reset_index(drop=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988dc9b5",
   "metadata": {},
   "source": [
    "Once we have to whom each team connects, we can build the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e58aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEdges(row):\n",
    "  edges = []\n",
    "  for connection in row[\"connections\"]:\n",
    "    edges.append([row[\"team_name\"], connection])\n",
    "  \n",
    "  return edges\n",
    "\n",
    "team_graph = nx.Graph()\n",
    "\n",
    "team_graph.add_nodes_from(df[\"team_name\"])\n",
    "\n",
    "all_edges = df.progress_apply(getEdges, axis=1)\n",
    "\n",
    "for edges in all_edges:\n",
    "  team_graph.add_edges_from(edges)\n",
    "\n",
    "\n",
    "print(f\"The team graph has {len(team_graph.nodes)} nodes and {len(team_graph.edges)} edges.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9536905",
   "metadata": {},
   "source": [
    "Now we can show multiple graph properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd103791",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Graph basic stats:\")\n",
    "print(f\"\\tNumber of nodes: {len(team_graph.nodes)}\")\n",
    "print(f\"\\tNumber of edges: {len(team_graph.edges)}\")\n",
    "print(f\"\\tAverage degree: {sum(x[1] for x in team_graph.degree)/len(team_graph.degree):.2f}\")\n",
    "print()\n",
    "print(f\"\\tMost connected node: {max(team_graph.degree, key=lambda x: x[1])[0]} \\\n",
    "with a degree of {max(team_graph.degree, key=lambda x: x[1])[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679b1a92",
   "metadata": {},
   "source": [
    "And we can reduce the size by getting only the Giant Connected Component (GCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b4089f",
   "metadata": {},
   "outputs": [],
   "source": [
    "team_graph_gcc = team_graph.subgraph(max(nx.connected_components(team_graph), key=len))\n",
    "\n",
    "print(f\"The team graph GCC has {len(team_graph_gcc.nodes)} nodes and {len(team_graph_gcc.edges)} edges.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b046e288",
   "metadata": {},
   "source": [
    "From this GCC, we can plot the graph and the degree distribution and check if it is random, or if it follows a powerlaw distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c29c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "forceatlas2 = ForceAtlas2(\n",
    "                          # Behavior alternatives\n",
    "                          outboundAttractionDistribution=False,  # Dissuade hubs\n",
    "                          linLogMode=False,  # NOT IMPLEMENTED\n",
    "                          adjustSizes=False,  # Prevent overlap (NOT IMPLEMENTED)\n",
    "                          edgeWeightInfluence=1.0,\n",
    "\n",
    "                          # Performance\n",
    "                          jitterTolerance=5.0,  # Tolerance\n",
    "                          barnesHutOptimize=False,\n",
    "                          barnesHutTheta=1.2,\n",
    "                          multiThreaded=False,  # NOT IMPLEMENTED\n",
    "\n",
    "                          # Tuning\n",
    "                          scalingRatio=10.0,\n",
    "                          strongGravityMode=False,\n",
    "                          gravity=100.0,\n",
    "\n",
    "                          # Log\n",
    "                          verbose=True)\n",
    "positions = forceatlas2.forceatlas2_networkx_layout(team_graph_gcc, pos=None, iterations=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b62851",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = []\n",
    "alphas = []\n",
    "colors = []\n",
    "max_degree = max(team_graph_gcc.degree(), key=lambda x: x[1])[1]\n",
    "\n",
    "for node in tqdm(team_graph_gcc.nodes):\n",
    "  size = team_graph_gcc.degree(node) * 20 + 50\n",
    "  alpha = max([team_graph_gcc.degree(node)/max_degree, .2])\n",
    "  \n",
    "  colors.append((random.random(), random.random(), random.random()))\n",
    "  \n",
    "  sizes.append(size)\n",
    "  alphas.append(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce52928",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(30, 30))\n",
    "\n",
    "nx.draw_networkx_nodes(team_graph_gcc,\n",
    "                       positions,\n",
    "                       linewidths  = 1,\n",
    "                       node_size   = sizes,\n",
    "                       node_color  = colors,\n",
    "                       alpha       = alphas,\n",
    "                       ax          = ax\n",
    "                      )\n",
    "\n",
    "nx.draw_networkx_edges(team_graph_gcc,\n",
    "                       positions,\n",
    "                       edge_color  = \"black\",\n",
    "                       arrowstyle  = \"-\",\n",
    "                       alpha       = 0.5,\n",
    "                       width       = .5,\n",
    "                       ax          = ax\n",
    "                      )  \n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf87645",
   "metadata": {},
   "source": [
    "The plot of the network does not give too much information. We can see that there are many nodes with a low degree (the ones from the border) and that there are a lot of nodes interconnected, but we cannot say much more only with this. To understand the network more, we do the degree distribution analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d22bb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax, l_ax), (ax_bp, ax_bp_l)) = plt.subplots(2, 2, figsize=(20, 12))\n",
    "fig.suptitle(\"Degree distribution\")\n",
    "\n",
    "degrees = dict(team_graph_gcc.degree()).values()\n",
    "\n",
    "hist, bins = np.histogram(np.array(list(degrees)), bins=500)\n",
    "center = (bins[:-1] + bins[1:])/2\n",
    "\n",
    "\n",
    "ax.plot(center, hist)\n",
    "ax.set_title(\"Degree distribution\")\n",
    "ax.set_xlabel(\"Degree\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "\n",
    "l_ax.plot(center, hist)\n",
    "l_ax.set_title(\"Degree distribution (log)\")\n",
    "l_ax.set_xlabel(\"Degree\")\n",
    "l_ax.set_ylabel(\"Count\")\n",
    "l_ax.set_xscale(\"log\")\n",
    "l_ax.set_yscale(\"log\")\n",
    "\n",
    "ax_bp.boxplot(degrees, vert=False, labels=[\"Degree\"])\n",
    "ax_bp.set_title(\"Box plot of the Degree\")\n",
    "ax_bp.set_xlabel(\"Degree\")\n",
    "\n",
    "ax_bp_l.boxplot(degrees, vert=False, labels=[\"Degree\"])\n",
    "ax_bp_l.set_title(\"Box plot of the Degree (log)\")\n",
    "ax_bp_l.set_xlabel(\"Degree\")\n",
    "ax_bp_l.set_xscale(\"log\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24128481",
   "metadata": {},
   "source": [
    "It does look like a powerlaw distribution, where a few nodes have a high degree while the majority of nodes only have a couple connections, but we need to prove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3ccd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = powerlaw.Fit(list(degrees))\n",
    "print(f\"The alpha of the degree dinstribution is: {results.power_law.alpha}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54e7bd6",
   "metadata": {},
   "source": [
    "The alpha is $>1$, so this network belongs to the **Superlinear Regime** which means that, on top of not being a random networks, it has a few disproportionaly atractive nodes, which group most of the data.\n",
    "\n",
    "### Wiki content\n",
    "\n",
    "Other metrics we can get from the data, is the lenght and content of their wikis. For that, first we need to get the content from them.\n",
    "\n",
    "The query looks lisk:\n",
    "\n",
    "```\n",
    "http://marvel.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=TEAM_NAME&format=json\n",
    "```\n",
    "\n",
    "Again the function is a bit long, but is done this way so we can do multithreading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194dc009",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_path = \"../data/team_content/\"\n",
    "\n",
    "links  = re.compile(r\"\\[\\[(.*?)(?:\\|.*?)?\\]\\]\")\n",
    "\n",
    "\n",
    "def get_team_content(names):\n",
    "    # Querying wiki for name\n",
    "    base_url = \"http://marvel.fandom.com/api.php?\"\n",
    "    action = \"action=query\"\n",
    "    title = f\"titles={'|'.join([urllib.parse.quote_plus(name.replace(' ', '_')) for name in names])}\"\n",
    "    content = \"prop=revisions&rvprop=content&rvslots=*\"\n",
    "    dataformat =\"format=json\"\n",
    "\n",
    "    query = \"{}{}&{}&{}&{}\".format(base_url, action, content, title, dataformat)\n",
    "    \n",
    "    resp = urllib.request.urlopen(query)\n",
    "    text = resp.read().decode(\"utf-8\")\n",
    "    data = json.loads(text)\n",
    "    \n",
    "    pages = data[\"query\"][\"pages\"]\n",
    "    for page_id, page in pages.items():\n",
    "        page[\"page_id\"] = page_id\n",
    "        filename = utils.generate_filename(page[\"title\"])\n",
    "        \n",
    "        with open(f\"{content_path}{filename}.json\", \"w\") as f:\n",
    "            json.dump(page, f, indent=4)\n",
    "\n",
    "def get_team_info(name, all_names_set):\n",
    "    filename = utils.generate_filename(name)\n",
    "    data = json.loads(open(f\"{content_path}{filename}.json\", \"r\").read())\n",
    "\n",
    "    content = data[\"revisions\"][0][\"slots\"][\"main\"][\"*\"]\n",
    "    \n",
    "    return content\n",
    "\n",
    "# Multithreaded solution for faster speed\n",
    "def get_content(names, max_workers = 16):\n",
    "    files = set(os.listdir(content_path))\n",
    "    missing_names = list(\n",
    "        filter(lambda x: f\"{utils.generate_filename(x)}.json\" not in files,\n",
    "        names\n",
    "        ))\n",
    "\n",
    "    if len(missing_names) == 0:\n",
    "        print(\"No missing names found ðŸ˜Š\")\n",
    "        return\n",
    "        \n",
    "    chunks = utils.generate_chunks(missing_names)\n",
    "    print(f\"Generated {len(chunks)} chunks!\")\n",
    "    with tqdm(total=len(chunks)) as pbar:\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "            futures = [ex.submit(get_team_content, chunk) \n",
    "                       for chunk in chunks]\n",
    "            for future in as_completed(futures):\n",
    "                pbar.update(1)\n",
    "    print(\"Done downloading files!\")\n",
    "\n",
    "def get_teams_infos(names, max_workers = 16):\n",
    "    all_names_set = set(names)\n",
    "    infos = []\n",
    "    with tqdm(total=len(names)) as pbar:\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "            for result in ex.map(get_team_info, names, itertools.repeat(all_names_set)):\n",
    "                infos.append(result)\n",
    "                pbar.update(1)\n",
    "    print(\"Done getting character information\")\n",
    "    return infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dd0ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_content(df[\"team_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c607ad06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"wiki_content\"] = get_teams_infos(df[\"team_name\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85add7fa",
   "metadata": {},
   "source": [
    "Once we have the wiki, we can, for example, get the number of words, number of unique words, lexical richeness and wordclouds of the top 20 teams whith most unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876b7eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLexicalRichnessWiki(row):\n",
    "  regex_links = r\"(?:\\{\\{.*?\\}\\}|\\[\\[.*?\\]\\])\"\n",
    "  regex_newline = r\"\\\\n\"\n",
    "  regex_no_alpha = r\"[^a-zA-Z ]\"\n",
    "  regex_aditional_space = r\"\\s\\s+\"\n",
    "  \n",
    "  text = \" \".join(row[\"wiki_content\"])\n",
    "  text = re.sub(regex_links, \"\", text)\n",
    "  text = re.sub(regex_newline, \" \", text)\n",
    "  text = re.sub(regex_no_alpha, \" \", text)\n",
    "  text = re.sub(regex_aditional_space, \" \", text)\n",
    "  \n",
    "  words = [word for word in text.lower().split(\" \") if len(word) > 1]\n",
    "  \n",
    "  number_words = len(words)\n",
    "  number_unique_words = len(list(set(words)))\n",
    "  lexical_richness = 0\n",
    "  \n",
    "  if number_words > 0:\n",
    "    lexical_richness = number_unique_words/number_words\n",
    "  return pd.Series([number_words, number_unique_words, lexical_richness])\n",
    "\n",
    "df[[\"number_words_wiki\",\n",
    "    \"number_unique_words_wiki\",\n",
    "    \"lexical_richness_wiki\"]] = df.progress_apply(getLexicalRichness, axis=1)\n",
    "\n",
    "df[df[\"number_unique_words_wiki\"] > 500].sort_values(\"lexical_richness_wiki\", ascending=False)\\\n",
    "                                        .reset_index(drop=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6d31a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df[df[\"number_unique_words_wiki\"] > 500].sort_values(\"%happy\", ascending=False)\\\n",
    "                                        .reset_index(drop=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2ff493",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "\n",
    "def preprocessWiki(row):\n",
    "  regex_links = r\"(?:\\{\\{.*?\\}\\}|\\[\\[.*?\\]\\])\"\n",
    "  regex_newline = r\"\\\\n\"\n",
    "  regex_no_alpha = r\"[^a-zA-Z ]\"\n",
    "  regex_aditional_space = r\"\\s\\s+\"\n",
    "  \n",
    "  content = row[\"wiki_content\"]\n",
    "  content = re.sub(regex_links, \" \", content)\n",
    "  content = re.sub(regex_newline, \" \", content)\n",
    "  content = re.sub(regex_no_alpha, \" \", content)\n",
    "  content = re.sub(regex_aditional_space, \" \", content)\n",
    "\n",
    "  tokens = tokenizer.tokenize(content)\n",
    "\n",
    "  all_words = [x.strip().lower() for x in tokens]\n",
    "\n",
    "  stop_words = list(nltk.corpus.stopwords.words(\"english\")) # Stopwords\n",
    "\n",
    "  filtered = [x for x in all_words if x not in stop_words]\n",
    "\n",
    "  lemmatized = [wnl.lemmatize(w) for w in filtered]\n",
    "  \n",
    "  new_stopwords = [\"im\", \"one\", \"earth\", \"know\", \"im\", \"b\", \"u\",\n",
    "                   \"vol\", \"jpg\", \"template\", \"database\", \"h\",\n",
    "                   \"e\", \"l\", \"title\", \"br\", \"imagesize\", \"px\",\n",
    "                   \"ref\", \"image\", \"pageid\", \"n\", \"revisions\"]\n",
    "\n",
    "  lemmatized = [x for x in lemmatized if x not in new_stopwords]\n",
    "  \n",
    "  return lemmatized\n",
    "\n",
    "df[\"processed_wiki\"] = df.progress_apply(preprocessWiki, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809cbeb9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "df_wiki = df.sort_values(\"number_unique_words_wiki\", ascending=False)\n",
    "\n",
    "\n",
    "word_cloud = WordCloud(max_words=2000,\n",
    "                       background_color=None,\n",
    "                       height=400,\n",
    "                       width=400,\n",
    "                      )\n",
    "\n",
    "for i in range(20):\n",
    "  content = \" \".join(df_wiki.iloc[i, 20])\n",
    "  word_cloud.generate(content).to_file(\"../data/wordclouds/\" + str(df_wiki.iloc[i, 0]) + \".png\")\n",
    "\n",
    "\n",
    "\n",
    "word_cloud = WordCloud(max_words=2000,\n",
    "                       background_color=\"white\"\n",
    "                      )\n",
    "fig, axarr = plt.subplots(10, 2, figsize=(20, 60))\n",
    "\n",
    "for i, ax in enumerate(axarr.flatten()):\n",
    "  content = \" \".join(df_wiki.iloc[i, 20])\n",
    "  ax.imshow(word_cloud.generate(content))\n",
    "  ax.axis(\"off\")\n",
    "  ax.set_title(f\"{df_wiki.iloc[i, 0]}, (Number members: {df_wiki.iloc[i, 2]})\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97ec846",
   "metadata": {},
   "source": [
    "### Correlation\n",
    "\n",
    "We can try to do some quick analysis using the correlation between the different columns, to see, for example, if having more members translates to a longer wikipage, or if the more lexical richness the quotes have, the more lexical richness the wiki page will have. To display this information we can use a heatmap, that summarize all the correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb1f83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, ax = plt.subplots(figsize=(15, 13))\n",
    "\n",
    "ax.set_title(\"Heatmap of the teams values correlation\")\n",
    "\n",
    "sns.heatmap(df.corr(), vmin=-1, vmax=1, annot=True, cmap=\"icefire\", ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e6e8a7",
   "metadata": {},
   "source": [
    "Yeah, we know it is pretty tricky to read, but here I can remark the most relevant results:\n",
    "\n",
    " - As expected, the **number of nodes** is highly correlated to the **number of connections**and is somehow related to the **number of quotes**, **number of words**, and **number of words in the wiki**.\n",
    " - Again, as expected, the **number of words** is highly correlated to the **number of unique words**, and **number of connections** and **number of unique words in the wiki**. \n",
    " - **Lexical richness** is related to **happy**, **sad**, and especially, **neutral**, and to the **lexical richness** of the wiki.\n",
    " \n",
    "## And back with characters\n",
    "\n",
    "We have analyzed the team information, done natural language processing over it, and seen a lot of stuff. It's time to jump into the characters and see what we can get from here.\n",
    "\n",
    "We already have the quotes from each character in our data frame. We can see that the character with more quotes is **Peter Parker (Spider-Man)**and that the top 10 characters with most quotes are all famous characters, but with only the name and not the alias, it is sometimes difficult to see who are they. Luckily, we can get their aliases from their wiki content, and now that we are at it, let's get their whole wiki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1a0e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char.sort_values(\"number_quotes\", ascending=False).reset_index(drop=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6028f0d8",
   "metadata": {},
   "source": [
    "We can get multiple pages at the same time. We can get the content, the links and number of connections, and the alias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef546984",
   "metadata": {},
   "outputs": [],
   "source": [
    "links  = re.compile(r\"\\[\\[(.*?)(?:\\|.*?)?\\]\\]\")\n",
    "\n",
    "content_path = \"../data/character_content/\"\n",
    "\n",
    "\n",
    "def get_character_content(names):\n",
    "    # Querying wiki for name\n",
    "    base_url = \"http://marvel.fandom.com/api.php?\"\n",
    "    action = \"action=query\"\n",
    "    title = f\"titles={'|'.join([urllib.parse.quote_plus(name.replace(' ', '_')) for name in names])}\"\n",
    "    content = \"prop=revisions&rvprop=content&rvslots=*\"\n",
    "    dataformat =\"format=json\"\n",
    "\n",
    "    query = \"{}{}&{}&{}&{}\".format(base_url, action, content, title, dataformat)\n",
    "    \n",
    "    resp = urllib.request.urlopen(query)\n",
    "    text = resp.read().decode(\"utf-8\")\n",
    "    data = json.loads(text)\n",
    "\n",
    "    pages = data[\"query\"][\"pages\"]\n",
    "    for page_id, page in pages.items():\n",
    "        page[\"page_id\"] = page_id\n",
    "        filename = utils.generate_filename(page[\"title\"])\n",
    "        with open(f\"{content_path}{filename}.json\", \"w\") as f:\n",
    "            json.dump(page, f, indent=4)\n",
    "\n",
    "# Multithreaded solution for faster speed\n",
    "def get_content(names, max_workers = 16):\n",
    "    files = set(os.listdir(content_path))\n",
    "    missing_names = list(\n",
    "        filter(lambda x: f\"{utils.generate_filename(x)}.json\" not in files,\n",
    "        names\n",
    "        ))\n",
    "\n",
    "    if len(missing_names) == 0:\n",
    "        print(\"No missing names found ðŸ˜Š\")\n",
    "        return\n",
    "        \n",
    "    chunks = utils.generate_chunks(missing_names)\n",
    "    print(f\"Generated {len(chunks)} chunks!\")\n",
    "    with tqdm(total=len(chunks)) as pbar:\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "            futures = [ex.submit(get_character_content, chunk) \n",
    "                       for chunk in chunks]\n",
    "            for future in as_completed(futures):\n",
    "                pbar.update(1)\n",
    "    print(\"Done downloading files!\")\n",
    "\n",
    "def GetCharacterInfo(row, all_names):\n",
    "  \n",
    "  all_names_set = set(all_names)\n",
    "  filename= utils.generate_filename(row[\"name\"])\n",
    "  \n",
    "  regex_alias = r\"\\|\\sCurrentAlias\\s+=\\s(.*?)(?:|\\|.*?)\\\\n\"\n",
    "  regex_alphanum = r\"[^a-zA-Z0-9\\- ]\"\n",
    "  regex_spaces = r\"\\s\\s+\"\n",
    "  alias = \"-\"\n",
    "\n",
    "  try:\n",
    "    with open(\"../data/character_content/\"+filename+\".json\") as f:\n",
    "      wiki = f.read()\n",
    "      alias = re.findall(regex_alias, wiki)[0]\n",
    "      alias = re.sub(regex_alphanum, \"\", alias)\n",
    "      alias = re.sub(regex_spaces, \" \", alias)\n",
    "      letters_alias = [x for x in alias if x != \" \"]\n",
    "      if len(letters_alias) == 0:\n",
    "        alias = \"-\"\n",
    "\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "  lin = links.findall(wiki)\n",
    "  lin = [x for x in lin if x in all_names_set and x != row[\"name\"]]\n",
    "\n",
    "  lin = list(set(lin))\n",
    "\n",
    "  return pd.Series([wiki, lin, len(lin),  alias.rstrip()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e1d5cf",
   "metadata": {},
   "source": [
    "As always, we will show the top 10 characters with more links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c5b1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_content(df_char[\"name\"])\n",
    "\n",
    "df_char[[\"wiki_content\",\n",
    "         \"links\",\n",
    "         \"number_links\",\n",
    "         \"alias\"]] = df_char.progress_apply(GetCharacterInfo, all_names=df_char[\"name\"], axis=1)\n",
    "\n",
    "df_char.sort_values(\"number_links\", ascending=False).reset_index(drop=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bbb949",
   "metadata": {},
   "source": [
    "Now we can see the alias, and the character are more easy to recognise :). Umm, there is a surprising character in here. On further inspect with Krakoa, which seems like an odd character for being the one with most links, we saw that it is a sentiment colony creature (don't ask, Marvel's universes are weird), where many other character live, and all of them are referenced in it's wiki. \n",
    "\n",
    "We can do some of the same analysis we did for the teams, but for characters. For example, we can check the lexical richness of each character. Now we are going to display the top 10 character by lexical richness where they have at least 100 unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029c3590",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char[[\"number_words\",\n",
    "         \"number_unique_words\",\n",
    "         \"lexical_richness\"]] = df_char.progress_apply(getLexicalRichness, axis=1)\n",
    "\n",
    "df_char[df_char[\"number_unique_words\"] > 100].sort_values(\"lexical_richness\", ascending=False)\\\n",
    "                                             .reset_index(drop=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9fe20e",
   "metadata": {},
   "source": [
    "We can do the same for the content, check which are the top 10 characters by lexical richness in their wiki, where they have at least 500 different words on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de820c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char[[\"number_words_wiki\",\n",
    "         \"number_unique_words_wiki\",\n",
    "         \"lexical_richness_wiki\"]] = df_char.progress_apply(getLexicalRichness, axis=1)\n",
    "\n",
    "df_char[df_char[\"number_unique_words_wiki\"] > 500].sort_values(\"lexical_richness_wiki\", ascending=False)\\\n",
    "                                        .reset_index(drop=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b47d88",
   "metadata": {},
   "source": [
    "Interesting. Now that we have a preliminary text analysis let's check the super cool wordclouds. First, let's do a word cloud for the top 20 characters with the most quotes.\n",
    "\n",
    "One important remark. When doing the wordclouds for a team, many had the word 'time'. Now, seeing the top 20 characters, they also have the word time, so we will remove it from the `content`, same for 'thing'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ed4100",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_char[\"processed_quotes\"] = df_char.progress_apply(preprocessQuotes, axis=1)\n",
    "\n",
    "fig, axarr = plt.subplots(10, 2, figsize=(20, 60))\n",
    "\n",
    "df_char_quotes = df_char.sort_values(\"number_unique_words\", ascending=False)\n",
    "\n",
    "word_cloud = WordCloud(max_words=2000,\n",
    "                       background_color=\"white\"\n",
    "                      )\n",
    "\n",
    "for i, ax in enumerate(axarr.flatten()):\n",
    "  content = \" \".join(word\n",
    "                     for quote in df_char_quotes.iloc[i, 15]\n",
    "                     for word in quote\n",
    "                     if word != \"time\"\n",
    "                     and word != \"thing\")\n",
    "  \n",
    "  ax.imshow(word_cloud.generate(content))\n",
    "  ax.axis(\"off\")\n",
    "  ax.set_title(f\"{df_char_quotes.iloc[i, 0]}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7902c9",
   "metadata": {},
   "source": [
    "We can do the wordclouds for the wiki content, and try to get if the general idea or sentiment is similar to their quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffcdb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072a3c0e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_char[\"processed_wiki\"] = df_char.progress_apply(preprocessWiki, axis=1)\n",
    "\n",
    "fig, axarr = plt.subplots(10, 2, figsize=(20, 60))\n",
    "\n",
    "df_char_wiki = df_char.sort_values(\"number_unique_words\", ascending=False)\n",
    "\n",
    "word_cloud = WordCloud(max_words=2000,\n",
    "                       background_color=\"white\"\n",
    "                      )\n",
    "\n",
    "for i, ax in enumerate(axarr.flatten()):\n",
    "  content = \" \".join(df_char_wiki.iloc[i, 16])\n",
    "  ax.imshow(word_cloud.generate(content))\n",
    "  ax.axis(\"off\")\n",
    "  ax.set_title(f\"{df_char_wiki.iloc[i, 0]}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b94b644",
   "metadata": {},
   "source": [
    "We can do the VADER sentiment too. Let's display the top 10 happiest and sadest that have at least 100 unique words in their quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1055c97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char[[\"%happy\",\n",
    "         \"%sad\",\n",
    "         \"%neutral\",\n",
    "         \"compound_sentiment\",\n",
    "         \"overall_category\"]] = df_char.progress_apply(getVaderSentiment, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3289e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char[df_char[\"number_unique_words\"] > 100].sort_values(\"compound_sentiment\", ascending=False)\\\n",
    "                                             .reset_index(drop=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44108dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char[df_char[\"number_unique_words\"] > 100].sort_values(\"compound_sentiment\", ascending=True)\\\n",
    "                                             .reset_index(drop=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3680af1c",
   "metadata": {},
   "source": [
    "### Graph\n",
    "\n",
    "Now it's time to build the graph again. Now the connections are going to be based on if they have references in the wiki. That means that, in this case, it will be a directed graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db58bb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_graph = nx.DiGraph()\n",
    "\n",
    "char_graph.add_nodes_from(df_char[\"name\"])\n",
    "for _, row in df_char.iterrows():\n",
    "  char_graph.add_edges_from([[row[\"name\"], x] for x in row[\"links\"]])\n",
    "  \n",
    "print(f\"The graph has {len(char_graph.nodes)} nodes and {len(char_graph.edges)} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08304f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Graph basic stats:\")\n",
    "print(f\"\\tNumber of nodes: {len(char_graph.nodes)}\")\n",
    "print(f\"\\tNumber of edges: {len(char_graph.edges)}\")\n",
    "print(f\"\\tAverage degree: {sum(x[1] for x in char_graph.degree)/len(char_graph.degree):.2f}\")\n",
    "print()\n",
    "print(f\"\\tMost connected node: {max(char_graph.degree, key=lambda x: x[1])[0]} \\\n",
    "with a degree of {max(char_graph.degree, key=lambda x: x[1])[1]}\")\n",
    "print(f\"\\tMost in connections node: {max(char_graph.in_degree, key=lambda x: x[1])[0]} \\\n",
    "with an in degree of {max(char_graph.in_degree, key=lambda x: x[1])[1]}\")\n",
    "print(f\"\\tMost out connections node: {max(char_graph.out_degree, key=lambda x: x[1])[0]} \\\n",
    "with an out degree of {max(char_graph.out_degree, key=lambda x: x[1])[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa924cf8",
   "metadata": {},
   "source": [
    "We can again obtain the GCC for the charachter graph, but for that we have to not have a directed graph, so we can do a non directed copy of the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20789f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_und_graph = nx.Graph(char_graph)\n",
    "\n",
    "char_graph_gcc = char_und_graph.subgraph(max(nx.connected_components(char_und_graph), key=len))\n",
    "\n",
    "print(f\"The character graph GCC has {len(char_graph_gcc.nodes)} nodes and {len(char_graph_gcc.edges)} edges.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751d44d7",
   "metadata": {},
   "source": [
    "And again, we can plot the graph, to see how it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fe7e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "forceatlas2 = ForceAtlas2(\n",
    "                          # Behavior alternatives\n",
    "                          outboundAttractionDistribution=False,  # Dissuade hubs\n",
    "                          linLogMode=False,  # NOT IMPLEMENTED\n",
    "                          adjustSizes=False,  # Prevent overlap (NOT IMPLEMENTED)\n",
    "                          edgeWeightInfluence=1.0,\n",
    "\n",
    "                          # Performance\n",
    "                          jitterTolerance=5.0,  # Tolerance\n",
    "                          barnesHutOptimize=False,\n",
    "                          barnesHutTheta=1.2,\n",
    "                          multiThreaded=False,  # NOT IMPLEMENTED\n",
    "\n",
    "                          # Tuning\n",
    "                          scalingRatio=10.0,\n",
    "                          strongGravityMode=False,\n",
    "                          gravity=100.0,\n",
    "\n",
    "                          # Log\n",
    "                          verbose=True)\n",
    "positions = forceatlas2.forceatlas2_networkx_layout(char_graph_gcc, pos=None, iterations=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09be9f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = []\n",
    "alphas = []\n",
    "colors = []\n",
    "max_degree = max(char_graph_gcc.degree(), key=lambda x: x[1])[1]\n",
    "\n",
    "for node in tqdm(char_graph_gcc.nodes):\n",
    "  size = char_graph_gcc.degree(node) * 20 + 50\n",
    "  alpha = max([char_graph_gcc.degree(node)/max_degree, .2])\n",
    "  \n",
    "  colors.append((random.random(), random.random(), random.random()))\n",
    "  \n",
    "  sizes.append(size)\n",
    "  alphas.append(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b9faac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(30, 30))\n",
    "\n",
    "nx.draw_networkx_nodes(char_graph_gcc,\n",
    "                       positions,\n",
    "                       linewidths  = 1,\n",
    "                       node_size   = sizes,\n",
    "                       node_color  = colors,\n",
    "                       alpha       = alphas,\n",
    "                       ax          = ax\n",
    "                      )\n",
    "\n",
    "nx.draw_networkx_edges(char_graph_gcc,\n",
    "                       positions,\n",
    "                       edge_color  = \"black\",\n",
    "                       arrowstyle  = \"-\",\n",
    "                       alpha       = 0.5,\n",
    "                       width       = .5,\n",
    "                       ax          = ax\n",
    "                      )  \n",
    "plt.axis(\"off\")\n",
    "ax.set_facecolor(None)\n",
    "\n",
    "plt.savefig(\"../docs/assets/img/networkimgs/character_network.png\", transparent=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a20ff23",
   "metadata": {},
   "source": [
    "Well :/, this is a huge hairball. Let's try to see if we can get any meaningfull information from the degree distribution analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfbac49",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ((ax, l_ax), (ax_bp, ax_bp_l)) = plt.subplots(2, 2, figsize=(20, 12))\n",
    "fig.suptitle(\"Degree distribution\")\n",
    "\n",
    "degrees = dict(char_graph.degree()).values()\n",
    "\n",
    "hist, bins = np.histogram(np.array(list(degrees)), bins=500)\n",
    "center = (bins[:-1] + bins[1:])/2\n",
    "\n",
    "\n",
    "ax.plot(center, hist)\n",
    "ax.set_title(\"Degree distribution\")\n",
    "ax.set_xlabel(\"Degree\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "\n",
    "l_ax.plot(center, hist)\n",
    "l_ax.set_title(\"Degree distribution (log)\")\n",
    "l_ax.set_xlabel(\"Degree\")\n",
    "l_ax.set_ylabel(\"Count\")\n",
    "l_ax.set_xscale(\"log\")\n",
    "l_ax.set_yscale(\"log\")\n",
    "\n",
    "ax_bp.boxplot(degrees, vert=False, labels=[\"Degree\"])\n",
    "ax_bp.set_title(\"Box plot of the Degree\")\n",
    "ax_bp.set_xlabel(\"Degree\")\n",
    "\n",
    "ax_bp_l.boxplot(degrees, vert=False, labels=[\"Degree\"])\n",
    "ax_bp_l.set_title(\"Box plot of the Degree (log)\")\n",
    "ax_bp_l.set_xlabel(\"Degree\")\n",
    "ax_bp_l.set_xscale(\"log\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig, ((ax, l_ax), (ax_bp, ax_bp_l)) = plt.subplots(2, 2, figsize=(20, 12))\n",
    "fig.suptitle(\"In Degree distribution\")\n",
    "\n",
    "degrees = dict(char_graph.in_degree()).values()\n",
    "\n",
    "hist, bins = np.histogram(np.array(list(degrees)), bins=500)\n",
    "center = (bins[:-1] + bins[1:])/2\n",
    "\n",
    "\n",
    "ax.plot(center, hist)\n",
    "ax.set_title(\"In Degree distribution\")\n",
    "ax.set_xlabel(\"Degree\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "\n",
    "l_ax.plot(center, hist)\n",
    "l_ax.set_title(\"In Degree distribution (log)\")\n",
    "l_ax.set_xlabel(\"Degree\")\n",
    "l_ax.set_ylabel(\"Count\")\n",
    "l_ax.set_xscale(\"log\")\n",
    "l_ax.set_yscale(\"log\")\n",
    "\n",
    "ax_bp.boxplot(degrees, vert=False, labels=[\"Degree\"])\n",
    "ax_bp.set_title(\"Box plot of the In Degree\")\n",
    "ax_bp.set_xlabel(\"Degree\")\n",
    "\n",
    "ax_bp_l.boxplot(degrees, vert=False, labels=[\"Degree\"])\n",
    "ax_bp_l.set_title(\"Box plot of the In Degree (log)\")\n",
    "ax_bp_l.set_xlabel(\"Degree\")\n",
    "ax_bp_l.set_xscale(\"log\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig, ((ax, l_ax), (ax_bp, ax_bp_l)) = plt.subplots(2, 2, figsize=(20, 12))\n",
    "fig.suptitle(\"Out Degree distribution\")\n",
    "\n",
    "degrees = dict(char_graph.out_degree()).values()\n",
    "\n",
    "hist, bins = np.histogram(np.array(list(degrees)), bins=500)\n",
    "center = (bins[:-1] + bins[1:])/2\n",
    "\n",
    "\n",
    "ax.plot(center, hist)\n",
    "ax.set_title(\"Out Degree distribution\")\n",
    "ax.set_xlabel(\"Degree\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "\n",
    "l_ax.plot(center, hist)\n",
    "l_ax.set_title(\"Out Degree distribution (log)\")\n",
    "l_ax.set_xlabel(\"Degree\")\n",
    "l_ax.set_ylabel(\"Count\")\n",
    "l_ax.set_xscale(\"log\")\n",
    "l_ax.set_yscale(\"log\")\n",
    "\n",
    "ax_bp.boxplot(degrees, vert=False, labels=[\"Degree\"])\n",
    "ax_bp.set_title(\"Box plot of the Out Degree\")\n",
    "ax_bp.set_xlabel(\"Degree\")\n",
    "\n",
    "ax_bp_l.boxplot(degrees, vert=False, labels=[\"Degree\"])\n",
    "ax_bp_l.set_title(\"Box plot of the Out Degree (log)\")\n",
    "ax_bp_l.set_xlabel(\"Degree\")\n",
    "ax_bp_l.set_xscale(\"log\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ff59fc",
   "metadata": {},
   "source": [
    "It does look like a powerlaw distribution, Let's prove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059fd6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = dict(char_graph.degree()).values()\n",
    "results = powerlaw.Fit(list(degrees), suppress_output=True)\n",
    "\n",
    "degrees_in = dict(char_graph.in_degree()).values()\n",
    "results_in = powerlaw.Fit(list(degrees_in), suppress_output=True)\n",
    "\n",
    "degrees_out = dict(char_graph.out_degree()).values()\n",
    "results_out = powerlaw.Fit(list(degrees_out), suppress_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef37ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The alpha of the degree dinstribution is: {results.power_law.alpha}\")\n",
    "print(f\"The alpha of the in degree dinstribution is: {results_in.power_law.alpha}\")\n",
    "print(f\"The alpha of the out degree dinstribution is: {results_out.power_law.alpha}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb7ea03",
   "metadata": {},
   "source": [
    "All the alphas are $>1$, so this network belongs to the **Superlinear Regime** which means that, on top of not being a random networks, it has a few disproportionaly atractive nodes, which group most of the data.\n",
    "\n",
    "We can get more information regarding the in and out degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617b080d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(x=degrees_in, y=degrees_out)\n",
    "plt.title(\"In degree vs. Out degree\")\n",
    "plt.xlabel(\"In degree\")\n",
    "plt.ylabel(\"Out degree\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65643eff",
   "metadata": {},
   "source": [
    "From this plot, we can see that most nodes with a low in-degree have a low out-degree too, and vice versa with a high degree. There are a few outliers, but they seem pretty correlated.\n",
    "\n",
    "\n",
    "## Conclusions\n",
    "From this project, we can first and foremost conclude that the Marvel Universe 616, and presumably the other marvel universes are very interlinked. \n",
    "\n",
    "We can from the network analysis observe that we have prominent characters linking to a lot of teams and a lot of characters. This is presumably because of the long history of the marvel comics and movies. \n",
    "\n",
    "As we have delved into mostly the comic universe, and as that universe contains so much more information than the cinematic, we can observe that the characters that are commonly known to some degree also are the characters most known (connected) in the comics: Wolverine, Spiderman, Magneto, etc. \n",
    "\n",
    "From the sentiment analysis, we observed that although some teams and characters are happier than others, the general happiness score is relatively low. This might be because the Marvel Universe is jam-packed with action. Most of the lore around the characters is about their struggles against supervillains or the descriptions of the supervillains mowing down planets. Although the narratives might be heroic or stoic, we still observed a lot of words that could have a negative connotation.\n",
    "\n",
    "Another lesson from this project is just how convoluted stories over time become. Initially, we wanted to divide the characters into heroes and villains. Still, as we progressed through the project, it became clear that the definition of good and evil is not as static in the Marvel Universe as one might imagine. The seed for us was reading up on Magneto, where he is first an ally of the X-Men and later an enemy. This, in our eyes, makes the universe a lot more interesting as the characters are no longer clearly black or white but greyer, and their actions and plotlines become more attractive as an effect of that.\n",
    "\n",
    "## Discussion\n",
    "\n",
    "We display multiple stats and characteristics from the data, but we were unable to reconstruct the original teams, as for that, more complex techniques would have been necessary. We started this proyect with an ambitious idea, without thinking if it was among the competences of this course, or how difficult would it be to achieve. The original idea was to recreate from the links the original teams. What's the problem, right? That shouldn't be so difficult. Well, the main problem is that, from the wiki content, one can get the characters that are referenced in other, but there is nop way, or at least no easy way, to get if the refrences from a character are allies, enemies, partners in a team or just random meetings between characters.\n",
    "\n",
    "The second problem is the complexity of the marvel timeline. Marvel comics have been around for quite some time (since 1939), and the main universe was created on 1961. Since then, many teams have been created and have disappeared. Many heores have been in multiple teams, and relationship between characters has changed a lot. Because of the size of the universe many different authors insert different ideas of what each character means to them. That results in characters that started in a team, then leave it, then came back and maybe, later, they became enemies to that team. This relationships results in that some characters are enemies and allies of themselves.\n",
    "\n",
    "As you can guess, this makes the team reconstruction goal almost impossible. \n",
    "\n",
    "After realising this, wqe thought that maybe we could try to insert some kind of timeline to the characters, to see how the evolution of the characters goes. But again, this is very difficult for many reasons. First, the time in the marvel universe is not constant at all. Peter Parker (Spider-Man) was introduced in 1962, but as today, he is still not over 30 years old. On top of that, some events have time travel and inmortal beings. It is a huge chaos.\n",
    "\n",
    "At the end, we decided to just analyze and do some kind of informative wiki of the marvel 616 universe, showing different stats for each character and each team, so we could apply different techniques and anlysis from what we learn during the course and show how dfficult and confusing the Marvel universe is.\n",
    "\n",
    "What is missing? Well, that is easy, a reconstruction of the teams based on the links from each character. Is an obtainable goal? Not really, at least not without using much more complex techniques.\n",
    "\n",
    "About possible improvements and future works from this proyect could be trying to incorpore more universes to the network. Apatrtrt from the 616, there are some more interesting and big universes from the marvel multiverse, such as *Earth-1610* (Ultimate Marvel Universe), Earth-199999 (Marvel Cinematic Universe) and Earth-928 (2099 year universe).\n",
    "\n",
    "\n",
    "## Contributions\n",
    "\n",
    "Althoug both of us have worked on all the proyect, I (Alejandro) have dedicated more time on this notebook, and have done more research on how to do the queries and how to obtain the analysis. Gustav is the one to thank for the multithreading of the download of the content, that allow us to get all the information in appropiate time (It went from around 10 hourst to download to 3 minutes, quite impressive to say the least :) ). He is the responsible of the cool website too.\n",
    "\n",
    "Both of us agreed on the idea on what to do, thorugh multiple sesions of brainstorming. Unfortunatly, we got stuck with the proyect multiple times, because we didn't know what to get from the data, as we could not do what we thought we could at first. But, after finishing the full proyect, we are happy with the results, and how everything turn out at the end, as it feel informative, and we have learnt quite a lot."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3405df15e4953545831bbb8174ce89dd7632c146c43537a8990c777f42b240d0"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
