{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Get all characters \n",
    "![header](../images/The_Marvel_Universe.png)\n",
    "\n",
    "In this first notebook we are going to substract all the characters from different Marvel heroes and villain teams to create the graph that is going to be used on the project"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import json\r\n",
    "import urllib.request\r\n",
    "\r\n",
    "import re\r\n",
    "\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "import os\r\n",
    "\r\n",
    "from tqdm.notebook import tqdm\r\n",
    "\r\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\r\n",
    "\r\n",
    "from itertools import repeat\r\n",
    "\r\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a6eb7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_characters(title):\n",
    "  \n",
    "  cmcontinue_text = \"\"\n",
    "  first_time = True\n",
    "  \n",
    "  character_list = []\n",
    "  \n",
    "  while cmcontinue_text or first_time: \n",
    "  \n",
    "    first_time = False\n",
    "  \n",
    "    baseurl = \"https://marvel.fandom.com/api.php?\"\n",
    "    action = \"action=query&list=categorymembers\"\n",
    "    q_title = \"cmtitle={}\".format(urllib.parse.quote_plus(title.replace(\" \", \"_\")))\n",
    "\n",
    "    content = \"prop=revisions&rvprop=content&rvslots=*\"\n",
    "    dataformat =\"format=json\"\n",
    "    cmcontinue = \"cmlimit=max&cmcontinue={}\".format(cmcontinue_text)\n",
    "\n",
    "    query = \"{}{}&{}&{}&{}&{}\".format(baseurl, action, q_title, content, dataformat, cmcontinue)\n",
    "    wikiresponse = urllib.request.urlopen(query)\n",
    "    wikidata = wikiresponse.read()\n",
    "    wikitext = wikidata.decode('utf-8')\n",
    "    \n",
    "    wiki_json = json.loads(wikitext)\n",
    "    \n",
    "    character_list += [character[\"title\"] for character in wiki_json[\"query\"][\"categorymembers\"]]\n",
    "\n",
    "    if \"continue\" in list(wiki_json.keys()):\n",
    "      cmcontinue_text = wiki_json[\"continue\"][\"cmcontinue\"]\n",
    "    else:\n",
    "      cmcontinue_text = \"\"\n",
    "      \n",
    "  return character_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120d3698",
   "metadata": {},
   "source": [
    "## Teams\n",
    "\n",
    "Instead of getting every single character from Marvel, we are going to work with a small subset. Why? Well, first of all, in the Marvel wiki there are more than 30.000 characters. Recopilate and process all that information would take a long time, and most of those characterrs are secondary characters that do not give much information.The second reason, and the deciding factor was that there is no easy way to get the link to all the characters from the wiki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2811a5bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_character_names(title: str):\r\n",
    "  \r\n",
    "  cmcontinue_text = \"\"\r\n",
    "  first_time = True\r\n",
    "  \r\n",
    "  character_list = []\r\n",
    "  \r\n",
    "  while cmcontinue_text or first_time: \r\n",
    "    first_time = False\r\n",
    "  \r\n",
    "    baseurl = \"https://marvel.fandom.com/api.php?\"\r\n",
    "    action = \"action=query&list=categorymembers\"\r\n",
    "    q_title = \"cmtitle={}\".format(urllib.parse.quote_plus(title.replace(\" \", \"_\")))\r\n",
    "\r\n",
    "    content = \"prop=revisions&rvprop=content&rvslots=*\"\r\n",
    "    dataformat =\"format=json\"\r\n",
    "    cmcontinue = \"cmlimit=max&cmcontinue={}\".format(cmcontinue_text)\r\n",
    "\r\n",
    "    query = \"{}{}&{}&{}&{}&{}\".format(baseurl, action, q_title, content, dataformat, cmcontinue)\r\n",
    "    wikiresponse = urllib.request.urlopen(query)\r\n",
    "    wikidata = wikiresponse.read()\r\n",
    "    wikitext = wikidata.decode('utf-8')\r\n",
    "    \r\n",
    "    wiki_json = json.loads(wikitext)\r\n",
    "    \r\n",
    "    character_list += [character[\"title\"] for character in wiki_json[\"query\"][\"categorymembers\"]]\r\n",
    "\r\n",
    "    if \"continue\" in list(wiki_json.keys()):\r\n",
    "      cmcontinue_text = wiki_json[\"continue\"][\"cmcontinue\"]\r\n",
    "    else:\r\n",
    "      cmcontinue_text = \"\"\r\n",
    "      \r\n",
    "  return character_list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Characters\r\n",
    "From the query above we are able to get all characters in the Marvel Universe. As the lenght of the list will show, there is a lot of characters! To somehow make the size of the network managable we have chosen to fixate on one specific universe - Universe 616, which can be considered to be the main storyline of the Marvel Universe. From this universe alone we find almost 30.000 unique characters.\r\n",
    "\r\n",
    "Using the `get_character_names()` we find all the character names. We then find the description for each character finding their current enemy and allied teams."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "character_names = get_character_names(\"Category:Earth-616/Characters\")"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Downloading character content\r\n",
    "Because our data set is so big (29.998) distinct characters, and the marvel wiki query functions are not the fastest, we have done some work to make the downloading faster. \r\n",
    "\r\n",
    "First we figured, that instead of doing a query for each title (character name), found in the `titles=`-option, we could query up to 50 characters at a time. \r\n",
    "\r\n",
    "The second optimization is that we utilized multithreading. At first we tried to use `async`-functions, but saw the opposite of a benefit, as the runtime increased. We hypothise, that either we made a mistake in the code, which we checked thouroughly, or the api was so fast, that asyncio actually slowed down the process. Either way, utlizing threads instead allowed us to go from a download time of 4 hours to 16 minutes!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "image  = re.compile(r\"Image\\s*=\\s(.*?)\\n\")\r\n",
    "gender = re.compile(\"Gender\\s*=\\s(.*?)\\n\")\r\n",
    "links  = re.compile(r\"\\[\\[(.*?)(?:\\|.*?)?\\]\\]\")\r\n",
    "\r\n",
    "content_path = \"../data/character_content/\"\r\n",
    "\r\n",
    "# Generating filenames, from original names, that are allowed as filenames\r\n",
    "def generate_filename(name):\r\n",
    "    name = name.replace(\" \", \"_\")\r\n",
    "    return \"\".join(x for x in name if x.isalnum() or x in [\"_\", \"(\", \")\", \"'\", \"-\", \".\"] )\r\n",
    "\r\n",
    "# Chunk size set at 50, as that as maximum allowed by \r\n",
    "def generate_chunks(names: list, chunk_size: int = 50) -> list[list]:\r\n",
    "    return [names[i:i+chunk_size] for i in range(0, len(names), chunk_size)]\r\n",
    "\r\n",
    "def get_character_content(names: list):\r\n",
    "    # Querying wiki for name\r\n",
    "    base_url = \"http://marvel.fandom.com/api.php?\"\r\n",
    "    action = \"action=query\"\r\n",
    "    title = f\"titles={'|'.join([urllib.parse.quote_plus(name.replace(' ', '_')) for name in names])}\"\r\n",
    "    content = \"prop=revisions&rvprop=content&rvslots=*\"\r\n",
    "    dataformat =\"format=json\"\r\n",
    "\r\n",
    "    query = \"{}{}&{}&{}&{}\".format(base_url, action, content, title, dataformat)\r\n",
    "    \r\n",
    "    resp = urllib.request.urlopen(query)\r\n",
    "    text = resp.read().decode(\"utf-8\")\r\n",
    "    data = json.loads(text)\r\n",
    "\r\n",
    "    pages = data[\"query\"][\"pages\"]\r\n",
    "    for page_id, page in pages.items():\r\n",
    "        page[\"page_id\"] = page_id\r\n",
    "        filename = generate_filename(page[\"title\"])\r\n",
    "        with open(f\"{content_path}{filename}.json\", \"w\") as f:\r\n",
    "            json.dump(page, f, indent=4)\r\n",
    "\r\n",
    "def get_character_info(name: str, all_names_set: set):\r\n",
    "    filename = generate_filename(name)\r\n",
    "    data = json.loads(open(f\"{content_path}{filename}.json\", \"r\").read())\r\n",
    "\r\n",
    "    title   = data[\"title\"]\r\n",
    "    content = data[\"revisions\"][0][\"slots\"][\"main\"][\"*\"]\r\n",
    "    pageid  = data[\"pageid\"]\r\n",
    "    \r\n",
    "    # Link for image for visualization in online\r\n",
    "    img = image.findall(content)\r\n",
    "    if len(img) >= 1: img = img[0]\r\n",
    "    \r\n",
    "    # Gender(s)\r\n",
    "    gen = gender.findall(content)\r\n",
    "    \r\n",
    "    # All square bracket links in the description\r\n",
    "    # Removing all links, that are not already an established character\r\n",
    "    # in our character_name_list, and is also not the name\r\n",
    "    # of the current person; no self loops!\r\n",
    "    lin = links.findall(content)\r\n",
    "    lin = [x for x in lin if x in all_names_set and x != title]\r\n",
    "\r\n",
    "    return title, pageid, img, gen, lin\r\n",
    "\r\n",
    "# Multithreaded solution for faster speed\r\n",
    "def get_content(names: list, max_workers: int = 16) -> None:\r\n",
    "    files = set(os.listdir(content_path))\r\n",
    "    missing_names = list(\r\n",
    "        filter(lambda x: f\"{generate_filename(x)}.json\" not in files,\r\n",
    "        names\r\n",
    "        ))\r\n",
    "\r\n",
    "    if len(missing_names) == 0:\r\n",
    "        print(\"No missing names found ðŸ˜Š\")\r\n",
    "        return\r\n",
    "        \r\n",
    "    chunks = generate_chunks(missing_names)\r\n",
    "    print(f\"Generated {len(chunks)} chunks!\")\r\n",
    "    with tqdm(total=len(chunks)) as pbar:\r\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as ex:\r\n",
    "            futures = [ex.submit(get_character_content, chunk) \r\n",
    "                       for chunk in chunks]\r\n",
    "            for future in as_completed(futures):\r\n",
    "                pbar.update(1)\r\n",
    "    print(\"Done downloading files!\")\r\n",
    "\r\n",
    "def get_character_infos(names: list, max_workers: int = 16) -> list:\r\n",
    "    all_names_set = set(names)\r\n",
    "    infos = []\r\n",
    "    with tqdm(total=len(names)) as pbar:\r\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as ex:\r\n",
    "            for result in ex.map(get_character_info, names, repeat(all_names_set)):\r\n",
    "                infos.append(result)\r\n",
    "                pbar.update(1)\r\n",
    "    print(\"Done getting character information\")\r\n",
    "    print(\"Converting to pd.DataFrame\")\r\n",
    "    return pd.DataFrame(infos, columns=[\"title\", \"pageid\", \"imglink\", \"gender\", \"links\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "get_content(character_names)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = get_character_infos(character_names)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.to_csv(f\"../data/marvel_characters.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('venv': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "interpreter": {
   "hash": "8506575580bf2c4cdf5283b234470f5904dd155f436b1a601ec65be85c4bd581"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
